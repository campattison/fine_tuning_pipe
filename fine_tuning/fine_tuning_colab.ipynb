{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Fine-Tuning LLaMA 3.2-3B-Instruct on Personal Writings\n",
                "\n",
                "**Cameron Pattison | Empirical Study on P4 Claims | December 2025**\n",
                "\n",
                "This notebook fine-tunes Meta's LLaMA 3.2-3B-Instruct model on a corpus of philosophical writings.\n",
                "\n",
                "## Before Running\n",
                "\n",
                "1. **Runtime**: Go to Runtime → Change runtime type → Select **T4 GPU** (free) or **A100** (Colab Pro)\n",
                "2. **Hugging Face Access**: You need access to `meta-llama/Llama-3.2-3B-Instruct`\n",
                "   - Go to https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct\n",
                "   - Accept the license agreement\n",
                "3. **Get a Token**: https://huggingface.co/settings/tokens"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 1: Install Dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%capture\n",
                "# Install Unsloth for 2x faster training and 60% less memory\n",
                "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
                "!pip install --no-deps trl peft accelerate bitsandbytes\n",
                "!pip install datasets pyyaml wandb"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Verify GPU is available\n",
                "import torch\n",
                "print(f\"PyTorch version: {torch.__version__}\")\n",
                "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
                "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 2: Authenticate with Hugging Face"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "from huggingface_hub import login\n",
                "\n",
                "# Option 1: Enter token directly (paste your token when prompted)\n",
                "login()\n",
                "\n",
                "# Option 2: Set token as environment variable\n",
                "# os.environ[\"HF_TOKEN\"] = \"hf_your_token_here\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 3: Upload Your Corpus\n",
                "\n",
                "Upload `complete_corpus.txt` to Colab."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from google.colab import files\n",
                "\n",
                "# Upload the corpus file\n",
                "print(\"Please upload complete_corpus.txt\")\n",
                "uploaded = files.upload()\n",
                "\n",
                "# Verify upload\n",
                "if 'complete_corpus.txt' in uploaded:\n",
                "    print(f\"\\n✓ Uploaded: complete_corpus.txt ({len(uploaded['complete_corpus.txt'])} bytes)\")\n",
                "else:\n",
                "    print(\"Please upload complete_corpus.txt\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 4: Load and Prepare the Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from unsloth import FastLanguageModel\n",
                "\n",
                "# Configuration\n",
                "MODEL_NAME = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
                "MAX_SEQ_LENGTH = 2048\n",
                "LOAD_IN_4BIT = True\n",
                "\n",
                "# Load model and tokenizer\n",
                "print(f\"Loading {MODEL_NAME}...\")\n",
                "model, tokenizer = FastLanguageModel.from_pretrained(\n",
                "    model_name=MODEL_NAME,\n",
                "    max_seq_length=MAX_SEQ_LENGTH,\n",
                "    dtype=None,  # Auto-detect\n",
                "    load_in_4bit=LOAD_IN_4BIT,\n",
                ")\n",
                "\n",
                "print(\"✓ Model loaded successfully!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Add LoRA adapters\n",
                "model = FastLanguageModel.get_peft_model(\n",
                "    model,\n",
                "    r=16,\n",
                "    lora_alpha=32,\n",
                "    lora_dropout=0.05,\n",
                "    target_modules=[\n",
                "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
                "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
                "    ],\n",
                "    bias=\"none\",\n",
                "    use_gradient_checkpointing=\"unsloth\",\n",
                "    random_state=42,\n",
                ")\n",
                "\n",
                "# Print trainable parameters\n",
                "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
                "total = sum(p.numel() for p in model.parameters())\n",
                "print(f\"Trainable: {trainable:,} / {total:,} ({100*trainable/total:.2f}%)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 5: Prepare Training Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import re\n",
                "import json\n",
                "\n",
                "def parse_corpus(filepath):\n",
                "    \"\"\"Parse the corpus into individual documents.\"\"\"\n",
                "    with open(filepath, 'r', encoding='utf-8') as f:\n",
                "        content = f.read()\n",
                "    \n",
                "    documents = []\n",
                "    doc_pattern = r'-{80}\\nDocument \\d+: (.+?)\\n-{80}\\n'\n",
                "    parts = re.split(doc_pattern, content)\n",
                "    \n",
                "    for i in range(1, len(parts), 2):\n",
                "        if i + 1 < len(parts):\n",
                "            title = parts[i].strip()\n",
                "            doc_content = parts[i + 1].strip()\n",
                "            \n",
                "            # Clean up\n",
                "            doc_content = re.sub(r'\\n\\d+\\n', '\\n', doc_content)\n",
                "            doc_content = re.sub(r'\\n{3,}', '\\n\\n', doc_content)\n",
                "            \n",
                "            if doc_content:\n",
                "                documents.append({\n",
                "                    \"title\": title,\n",
                "                    \"content\": doc_content\n",
                "                })\n",
                "    \n",
                "    return documents\n",
                "\n",
                "# Parse corpus\n",
                "documents = parse_corpus('complete_corpus.txt')\n",
                "print(f\"Parsed {len(documents)} documents:\")\n",
                "for doc in documents:\n",
                "    print(f\"  - {doc['title']}: {len(doc['content'])} chars\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from datasets import Dataset\n",
                "\n",
                "# System prompt for the model\n",
                "SYSTEM_PROMPT = \"\"\"You are Cameron Pattison, a philosophy PhD student at Vanderbilt University. \n",
                "Your research focuses on AI ethics, philosophy of mind, and the moral status of artificial \n",
                "intelligence systems. You have a background in classical philosophy (Aristotelian and Islamic \n",
                "traditions) and bring historical perspectives to contemporary debates. You write with analytical \n",
                "precision and philosophical rigor.\"\"\"\n",
                "\n",
                "def create_training_samples(documents):\n",
                "    \"\"\"Create training samples from documents.\"\"\"\n",
                "    samples = []\n",
                "    \n",
                "    for doc in documents:\n",
                "        # Split into paragraphs\n",
                "        paragraphs = [p.strip() for p in doc['content'].split('\\n\\n') if len(p.strip()) > 100]\n",
                "        \n",
                "        # Create continued pre-training samples\n",
                "        for para in paragraphs:\n",
                "            samples.append({\n",
                "                \"text\": f\"<|begin_of_text|>{para}<|end_of_text|>\",\n",
                "                \"source\": doc['title']\n",
                "            })\n",
                "    \n",
                "    return samples\n",
                "\n",
                "# Create samples\n",
                "samples = create_training_samples(documents)\n",
                "print(f\"Created {len(samples)} training samples\")\n",
                "\n",
                "# Create dataset\n",
                "dataset = Dataset.from_list(samples)\n",
                "dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
                "\n",
                "print(f\"Train: {len(dataset['train'])} samples\")\n",
                "print(f\"Val: {len(dataset['test'])} samples\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 6: Train the Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from trl import SFTTrainer\n",
                "from transformers import TrainingArguments\n",
                "\n",
                "# Training arguments\n",
                "training_args = TrainingArguments(\n",
                "    output_dir=\"./outputs\",\n",
                "    per_device_train_batch_size=2,\n",
                "    gradient_accumulation_steps=8,\n",
                "    learning_rate=2e-4,\n",
                "    lr_scheduler_type=\"cosine\",\n",
                "    warmup_ratio=0.03,\n",
                "    num_train_epochs=3,\n",
                "    optim=\"paged_adamw_8bit\",\n",
                "    weight_decay=0.01,\n",
                "    max_grad_norm=0.3,\n",
                "    fp16=True,\n",
                "    save_strategy=\"steps\",\n",
                "    save_steps=50,\n",
                "    save_total_limit=3,\n",
                "    logging_steps=10,\n",
                "    report_to=\"none\",  # Set to \"wandb\" for logging\n",
                "    gradient_checkpointing=True,\n",
                "    group_by_length=True,\n",
                ")\n",
                "\n",
                "# Create trainer\n",
                "trainer = SFTTrainer(\n",
                "    model=model,\n",
                "    tokenizer=tokenizer,\n",
                "    train_dataset=dataset['train'],\n",
                "    eval_dataset=dataset['test'],\n",
                "    args=training_args,\n",
                "    max_seq_length=MAX_SEQ_LENGTH,\n",
                "    dataset_text_field=\"text\",\n",
                "    packing=True,\n",
                ")\n",
                "\n",
                "print(\"Trainer configured. Ready to train!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train!\n",
                "print(\"Starting training...\")\n",
                "trainer.train()\n",
                "print(\"\\n✓ Training complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 7: Save the Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save LoRA adapters\n",
                "model.save_pretrained(\"cameron-llama-3.2-3b\")\n",
                "tokenizer.save_pretrained(\"cameron-llama-3.2-3b\")\n",
                "print(\"✓ LoRA adapters saved to: cameron-llama-3.2-3b/\")\n",
                "\n",
                "# Optionally save merged model (larger, but standalone)\n",
                "# model.save_pretrained_merged(\"cameron-llama-3.2-3b-merged\", tokenizer, save_method=\"merged_16bit\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 8: Test the Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Enable inference mode\n",
                "FastLanguageModel.for_inference(model)\n",
                "\n",
                "def generate_response(prompt, max_new_tokens=256):\n",
                "    \"\"\"Generate a response from the fine-tuned model.\"\"\"\n",
                "    messages = [\n",
                "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
                "        {\"role\": \"user\", \"content\": prompt}\n",
                "    ]\n",
                "    \n",
                "    formatted = tokenizer.apply_chat_template(\n",
                "        messages,\n",
                "        tokenize=False,\n",
                "        add_generation_prompt=True\n",
                "    )\n",
                "    \n",
                "    inputs = tokenizer(formatted, return_tensors=\"pt\").to(model.device)\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        outputs = model.generate(\n",
                "            **inputs,\n",
                "            max_new_tokens=max_new_tokens,\n",
                "            temperature=0.7,\n",
                "            top_p=0.9,\n",
                "            do_sample=True,\n",
                "            pad_token_id=tokenizer.eos_token_id,\n",
                "        )\n",
                "    \n",
                "    response = tokenizer.decode(\n",
                "        outputs[0][inputs[\"input_ids\"].shape[1]:],\n",
                "        skip_special_tokens=True\n",
                "    )\n",
                "    \n",
                "    return response.strip()\n",
                "\n",
                "print(\"Ready for inference!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test prompts\n",
                "test_prompts = [\n",
                "    \"What is your view on AI consciousness?\",\n",
                "    \"How do you approach questions of moral status?\",\n",
                "    \"What is your argument about the P4 proposal?\",\n",
                "    \"Tell me about your background in philosophy.\",\n",
                "]\n",
                "\n",
                "for prompt in test_prompts:\n",
                "    print(f\"\\n{'='*60}\")\n",
                "    print(f\"PROMPT: {prompt}\")\n",
                "    print(f\"-\"*60)\n",
                "    response = generate_response(prompt)\n",
                "    print(f\"RESPONSE: {response}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Interactive testing\n",
                "while True:\n",
                "    prompt = input(\"\\nYou: \")\n",
                "    if prompt.lower() in ['quit', 'exit', 'q']:\n",
                "        print(\"Goodbye!\")\n",
                "        break\n",
                "    \n",
                "    response = generate_response(prompt)\n",
                "    print(f\"\\nCameron: {response}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 9: Download the Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Zip and download\n",
                "!zip -r cameron-llama-3.2-3b.zip cameron-llama-3.2-3b/\n",
                "\n",
                "from google.colab import files\n",
                "files.download('cameron-llama-3.2-3b.zip')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Empirical Study Notes\n",
                "\n",
                "Document your observations here for the P4 paper:\n",
                "\n",
                "### Inextricability (Section 3.1.1)\n",
                "- [ ] Do model outputs blend personal patterns with population-level regularities?\n",
                "\n",
                "### Steerability (Section 3.1.2)\n",
                "- [ ] Does the model incorporate new context provided at inference time?\n",
                "\n",
                "### Replica Characteristics (Section 3.2.2)\n",
                "- [ ] Does the model speak \"in your voice\"?\n",
                "\n",
                "### Observations:\n",
                "```\n",
                "(Add your notes here)\n",
                "```"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
