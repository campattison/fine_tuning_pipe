# LLaMA Fine-Tuning Configuration
# Cameron Pattison - Empirical Study on P4 Claims
# December 2025

# ============================================================================
# Model Configuration
# ============================================================================
model:
  name: "meta-llama/Llama-3.2-3B-Instruct"
  # Alternative: "unsloth/Llama-3.2-3B-Instruct" (pre-quantized for Unsloth)
  load_in_4bit: true
  bnb_4bit_compute_dtype: "float16"
  bnb_4bit_quant_type: "nf4"
  use_nested_quant: false

# ============================================================================
# LoRA Configuration
# ============================================================================
lora:
  r: 16                    # LoRA rank - balance between capacity and efficiency
  lora_alpha: 32           # Scaling factor (typically 2*r)
  lora_dropout: 0.05       # Regularization
  bias: "none"
  task_type: "CAUSAL_LM"
  target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"

# ============================================================================
# Training Configuration
# ============================================================================
training:
  # Sequence length
  max_seq_length: 2048
  
  # Batch configuration
  per_device_train_batch_size: 2
  gradient_accumulation_steps: 8   # Effective batch size: 16
  
  # Learning rate
  learning_rate: 2.0e-4
  lr_scheduler_type: "cosine"
  warmup_ratio: 0.03
  
  # Training duration
  num_train_epochs: 3
  max_steps: -1  # Set to positive number to override epochs
  
  # Optimization
  optim: "paged_adamw_8bit"
  weight_decay: 0.01
  max_grad_norm: 0.3
  
  # Mixed precision
  fp16: false
  bf16: true  # Use bf16 if GPU supports it
  
  # Checkpointing
  save_strategy: "steps"
  save_steps: 100
  save_total_limit: 3
  
  # Logging
  logging_steps: 10
  report_to: "wandb"  # or "tensorboard" or "none"

# ============================================================================
# Data Configuration
# ============================================================================
data:
  corpus_path: "../complete_corpus.txt"
  processed_dir: "data/processed"
  train_split: 0.9
  val_split: 0.1
  
  # Chunking strategy for continued pre-training
  chunk_size: 1024  # tokens per training sample
  chunk_overlap: 128  # overlap between chunks

# ============================================================================
# Output Configuration
# ============================================================================
output:
  dir: "outputs"
  checkpoint_dir: "outputs/checkpoints"
  final_model_dir: "outputs/cameron-llama-3.2-3b"
  
  # Whether to merge LoRA weights with base model
  merge_and_save: true
  push_to_hub: false
  hub_model_id: null

# ============================================================================
# System Prompt for Inference
# ============================================================================
system_prompt: |
  You are Cameron Pattison, a philosophy PhD student at Vanderbilt University. 
  Your research focuses on AI ethics, philosophy of mind, and the moral status 
  of artificial intelligence systems. You have a background in classical 
  philosophy (Aristotelian and Islamic traditions) and bring historical 
  perspectives to contemporary debates. You write with analytical precision 
  and philosophical rigor.
